# -*- coding: utf-8 -*-
"""Heart Disease Classification.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1UeA0_AImiB5LYAVsIS8Hg022mCNZ7iDy
"""

# Install dependencies as needed:
# pip install kagglehub[pandas-datasets]
import kagglehub
from kagglehub import KaggleDatasetAdapter
import warnings

# Filter out DeprecationWarnings
warnings.filterwarnings("ignore", category=DeprecationWarning)

# Set the path to the file you'd like to load
file_path = "heart.csv"

# Load the latest version
df = kagglehub.load_dataset(KaggleDatasetAdapter.PANDAS,"johnsmith88/heart-disease-dataset",
  file_path,
  # Provide any additional arguments like
  # sql_query or pandas_kwargs. See the
  # documenation for more information:
  # https://github.com/Kaggle/kagglehub/blob/main/README.md#kaggledatasetadapterpandas
)

print("First 5 records:", df.head())
df.shape

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.metrics import classification_report
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score
from sklearn.model_selection import GridSearchCV
from sklearn.metrics import confusion_matrix

print("\nDataset Info:")
print(df.info())

print("\nMissing values:")
print(df.isnull().sum())

print("\nTarget distribution:")
print(df['target'].value_counts())

df.duplicated().sum()

df = df.drop_duplicates()
df.shape

X = df.drop(columns='target', axis=1)
Y = df['target']

df['target'].value_counts()


df.shape

X_train, X_test, Y_train,Y_test = train_test_split(X, Y, test_size=0.2, stratify=Y, random_state=42)

print(X.shape, X_train.shape, X_test.shape)

model = LogisticRegression(max_iter=1000)

model.fit(X_train, Y_train)

X_train_prediction = model.predict(X_train)
training_data_accuracy = accuracy_score(X_train_prediction, Y_train)

print('Accuracy on Training data : ', training_data_accuracy)

X_test_prediction = model.predict(X_test)
test_data_accuracy = accuracy_score(X_test_prediction, Y_test)

print('Accuracy on Test data : ', test_data_accuracy)

# Get predictions for the test set
X_test_prediction = model.predict(X_test)

# Print the classification report
print("\nClassification Report on Test Data:")
print(classification_report(Y_test, X_test_prediction))

plt.figure(figsize=(12, 10))
sns.heatmap(df.corr(), annot=True, cmap='coolwarm', fmt=".2f")
plt.title('Correlation Heatmap of Heart Disease Dataset')
plt.show()

plt.figure(figsize=(10, 6))
sns.regplot(x='age', y='thalach', data=df, scatter_kws={'alpha':0.5})
plt.title('Relationship between Age and Maximum Heart Rate (thalach)')
plt.xlabel('Age')
plt.ylabel('Maximum Heart Rate (thalach)')
plt.grid(True)
plt.show()

plt.figure(figsize=(10, 6))
sns.lineplot(x='age', y='thalach', data=df)
plt.title('Average Maximum Heart Rate by Age')
plt.xlabel('Age')
plt.ylabel('Average Maximum Heart Rate (thalach)')
plt.grid(True)
plt.show()

plt.figure(figsize=(10, 6))
sns.scatterplot(x='age', y='chol', data=df, alpha=0.5)
plt.title('Relationship between Age and Cholesterol')
plt.xlabel('Age')
plt.ylabel('Cholesterol')
plt.grid(True)
plt.show()

plt.figure(figsize=(10, 6))
sns.boxplot(x='sex', y='age', data=df)
plt.title('Distribution of Age by Sex')
plt.xlabel('Sex (0: Female, 1: Male)')
plt.ylabel('Age')
plt.xticks([0, 1], ['Female', 'Male'])
plt.show()

# Get predictions on the test set
X_test_prediction = model.predict(X_test)

# Calculate the confusion matrix
cm = confusion_matrix(Y_test, X_test_prediction)

# Plot the confusion matrix as a heatmap
plt.figure(figsize=(6, 4))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=False,
            xticklabels=['Predicted 0', 'Predicted 1'],
            yticklabels=['Actual 0', 'Actual 1'])
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.title('Confusion Matrix')
plt.show()

# Get the coefficients of the model
feature_importance = model.coef_[0]

# Create a dictionary of feature names and their importance scores
feature_names = X.columns
feature_importance_dict = dict(zip(feature_names, feature_importance))

# Sort the features by importance
sorted_feature_importance = sorted(feature_importance_dict.items(), key=lambda item: abs(item[1]), reverse=True)

print("Feature Importance (Absolute Coefficient Values for Logistic Regression):")
for feature, importance in sorted_feature_importance:
    print(f"{feature}: {importance:.4f}")

# Define the parameter grid
param_grid = {
    'C': [0.001, 0.01, 0.1, 1, 10, 100],
    'solver': ['liblinear', 'lbfgs']
}

# Create a GridSearchCV object
# Increased max_iter to address ConvergenceWarning
grid_search = GridSearchCV(LogisticRegression(max_iter=5000), param_grid, cv=5, scoring='accuracy')

# Fit the grid search to the data
grid_search.fit(X_train, Y_train)

# Print the best parameters and best score
print("Best parameters:", grid_search.best_params_)
print("Best cross-validation accuracy:", grid_search.best_score_)

# Get the best model
best_model = grid_search.best_estimator_

# Evaluate the best model on the test set
test_accuracy = best_model.score(X_test, Y_test)
print("Test accuracy with best model:", test_accuracy)